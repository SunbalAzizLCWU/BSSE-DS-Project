{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOv988bTQHCHSwsLgj2lKBO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunbalAzizLCWU/BSSE-DS-Project/blob/main/SunbalW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfwV_hyUJns0",
        "outputId": "902540cb-45b5-4b44-9abc-43f0ea8ec434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Part 1: Setting up Kaggle and Downloading Dataset ---\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "Kaggle API token installed.\n",
            "Dataset URL: https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification\n",
            "License(s): copyright-authors\n",
            "Downloading garbage-classification.zip to /content\n",
            "  0% 0.00/82.0M [00:00<?, ?B/s]\n",
            "100% 82.0M/82.0M [00:00<00:00, 1.43GB/s]\n",
            "\n",
            "Dataset downloaded. Unzipping...\n",
            "Dataset unzipped. Ready for preprocessing.\n"
          ]
        }
      ],
      "source": [
        "#Dataset Setup as always\n",
        "# -----------------------------------------------------------------------\n",
        "print(\"--- Part 1: Setting up Kaggle and Downloading Dataset ---\")\n",
        "\n",
        "# 1. Install Kaggle library\n",
        "!pip install kaggle\n",
        "\n",
        "# 2. Set up the Kaggle API token\n",
        "# (Ensure kaggle.json is uploaded to your Colab environment)\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "print(\"Kaggle API token installed.\")\n",
        "\n",
        "# 3. Download the dataset\n",
        "# This is the dataset for project 9: \"Image-based Waste Classification\"\n",
        "!kaggle datasets download -d asdasdasasdas/garbage-classification\n",
        "print(\"\\nDataset downloaded. Unzipping...\")\n",
        "\n",
        "# 4. Unzip the file\n",
        "!unzip -q garbage-classification.zip\n",
        "print(\"Dataset unzipped. Ready for preprocessing.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and Data Preprocessing\n",
        "print(\"\\n--- Part 2: Importing Libraries and Preprocessing Data ---\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- Import Week 6 Models ---\n",
        "from sklearn.linear_model import LogisticRegression # Our W5 baseline\n",
        "from sklearn.tree import DecisionTreeClassifier # Class Task\n",
        "from sklearn.ensemble import RandomForestClassifier # Class Task & Assignment\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --- Preprocessing (Identical to Week 5) ---\n",
        "\n",
        "# Define constants\n",
        "data_dir = 'Garbage classification/Garbage classification'\n",
        "classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
        "IMG_SIZE = 64 # Must be same as W5 (64x64) for a fair comparison\n",
        "\n",
        "# Lists to hold our data\n",
        "X_data = [] # This will hold the flattened image data\n",
        "y_data = [] # This will hold the labels\n",
        "\n",
        "def preprocess_images():\n",
        "    \"\"\"\n",
        "    Loops through all images, resizes them, flattens them,\n",
        "    and returns two numpy arrays: X (data) and y (labels).\n",
        "    \"\"\"\n",
        "    print(f\"\\nStarting image preprocessing from {data_dir}...\")\n",
        "    for class_name in classes:\n",
        "        class_dir_path = os.path.join(data_dir, class_name)\n",
        "        class_label = classes.index(class_name)\n",
        "\n",
        "        if not os.path.isdir(class_dir_path):\n",
        "            print(f\"Warning: Directory not found {class_dir_path}\")\n",
        "            continue\n",
        "\n",
        "        for image_file in os.listdir(class_dir_path):\n",
        "            image_path = os.path.join(class_dir_path, image_file)\n",
        "            try:\n",
        "                # Open image, convert to grayscale, and resize\n",
        "                img = Image.open(image_path).convert('L') # 'L' = grayscale\n",
        "                img_resized = img.resize((IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "                # Flatten the 64x64 image into a 1D vector of 4096 pixels\n",
        "                img_vector = np.array(img_resized).flatten()\n",
        "\n",
        "                X_data.append(img_vector)\n",
        "                y_data.append(class_label)\n",
        "\n",
        "            except Exception as e:\n",
        "                # Skip corrupted files [cite: 216-217]\n",
        "                print(f\"Skipping corrupted file: {image_path} | Error: {e}\")\n",
        "\n",
        "    print(\"Image preprocessing complete.\")\n",
        "    return np.array(X_data), np.array(y_data)\n",
        "\n",
        "# Run the function\n",
        "X, y = preprocess_images()\n",
        "\n",
        "print(f\"\\nData shape (X): {X.shape}\")\n",
        "print(f\"Labels shape (y): {y.shape}\")\n",
        "\n",
        "# Scale the data\n",
        "print(\"\\nScaling data (StandardScaler)...\")\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(\"Data scaling complete.\")\n",
        "\n",
        "# --- Train/Test Split (Identical to Week 5) ---\n",
        "print(\"\\nSplitting data into 80% train and 20% test sets...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"Training images: {X_train.shape[0]}\")\n",
        "print(f\"Testing images: {X_test.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wth-fMosLZLM",
        "outputId": "e6b8365e-b3d6-4627-ee38-fc3a35cfaff1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Part 2: Importing Libraries and Preprocessing Data ---\n",
            "\n",
            "Starting image preprocessing from Garbage classification/Garbage classification...\n",
            "Image preprocessing complete.\n",
            "\n",
            "Data shape (X): (2527, 4096)\n",
            "Labels shape (y): (2527,)\n",
            "\n",
            "Scaling data (StandardScaler)...\n",
            "Data scaling complete.\n",
            "\n",
            "Splitting data into 80% train and 20% test sets...\n",
            "Training images: 2021\n",
            "Testing images: 506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Week 6 Model Training & Comparison\n",
        "print(\"\\n--- Part 3: Training and Evaluating Week 6 Models ---\")\n",
        "\n",
        "# --- Model 1: Logistic Regression (Baseline from W5) ---\n",
        "print(\"Training Model 1: Logistic Regression...\")\n",
        "model_logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_logreg.fit(X_train, y_train)\n",
        "y_pred_logreg = model_logreg.predict(X_test)\n",
        "acc_logreg = accuracy_score(y_test, y_pred_logreg)\n",
        "print(\"Logistic Regression training complete.\")\n",
        "\n",
        "# --- Model 2: Decision Tree (Class Task) ---\n",
        "print(\"Training Model 2: Decision Tree...\")\n",
        "model_tree = DecisionTreeClassifier(random_state=42)\n",
        "model_tree.fit(X_train, y_train)\n",
        "y_pred_tree = model_tree.predict(X_test)\n",
        "acc_tree = accuracy_score(y_test, y_pred_tree)\n",
        "print(\"Decision Tree training complete.\")\n",
        "\n",
        "# --- Model 3: Random Forest (Class Task & Assignment) ---\n",
        "print(\"Training Model 3: Random Forest... (This may take a minute)\")\n",
        "model_rf = RandomForestClassifier(random_state=42)\n",
        "model_rf.fit(X_train, y_train)\n",
        "y_pred_rf = model_rf.predict(X_test)\n",
        "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(\"Random Forest training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAzdF8q0Lmnj",
        "outputId": "0b5f50fe-2f64-4c56-bf0a-1997a5864c8b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Part 3: Training and Evaluating Week 6 Models ---\n",
            "Training Model 1: Logistic Regression...\n",
            "Logistic Regression training complete.\n",
            "Training Model 2: Decision Tree...\n",
            "Decision Tree training complete.\n",
            "Training Model 3: Random Forest... (This may take a minute)\n",
            "Random Forest training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Assignment 6 Report\n",
        "\n",
        "print(\"\\n=======================================================\")\n",
        "print(\"      Week 6 Assignment: Model Comparison Report\")\n",
        "print(\"=======================================================\")\n",
        "print(\"Comparing accuracy of all trained models on the test set:\\n\")\n",
        "print(f\"1. Logistic Regression (Baseline): {acc_logreg * 100:.2f}%\")\n",
        "print(f\"2. Decision Tree:                  {acc_tree * 100:.2f}%\")\n",
        "print(f\"3. Random Forest:                  {acc_rf * 100:.2f}%\")\n",
        "print(\"\\n--- Week 6 Tasks Complete ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4k4g2dSLupM",
        "outputId": "afe3c87a-5963-431e-99be-e29792175e02"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=======================================================\n",
            "      Week 6 Assignment: Model Comparison Report\n",
            "=======================================================\n",
            "Comparing accuracy of all trained models on the test set:\n",
            "\n",
            "1. Logistic Regression (Baseline): 31.42%\n",
            "2. Decision Tree:                  41.90%\n",
            "3. Random Forest:                  63.64%\n",
            "\n",
            "--- Week 6 Tasks Complete ---\n"
          ]
        }
      ]
    }
  ]
}